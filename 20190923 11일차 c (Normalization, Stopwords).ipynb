{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지난 시간 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'd like to learn more somthing. \\\n",
    "            I'd like to learn more somthing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'d\", 'like', 'to', 'learn', 'more', 'somthing', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sent_tokenize: 문장단위로 쪼개준다.\n",
    "# word_tokenize: 단어단위로 쪼개준다.\n",
    "\n",
    "word_tokenize(sent_tokenize(sentence)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'd like to learn more somthing. \\\n",
    "            I'd like to learn more somthing. \\\n",
    "            what're i'm, isn't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['what', \"'re\", 'i', \"'m\", ',', 'is', \"n't\"],\n",
       " [('what', 'WP'),\n",
       "  (\"'re\", 'VBP'),\n",
       "  ('i', 'JJ'),\n",
       "  (\"'m\", 'VBP'),\n",
       "  (',', ','),\n",
       "  ('is', 'VBZ'),\n",
       "  (\"n't\", 'RB')])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS(Part of Speech): 품사\n",
    "# pos_tag: 품사에 태그 달아줌 / 동음이의어를 구분할 수 있게 됨.\n",
    "\n",
    "word_tokenize(sent_tokenize(sentence)[-1]), \\\n",
    "pos_tag(word_tokenize(sent_tokenize(sentence)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'우리나라\", \"'\", '의', '국화는', '무궁화', '!', '입니다', '.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글은 지원하지 않기때문에 단순한 기준(공백, punkt)에 따라서 split함\n",
    "\n",
    "word_tokenize(\"'우리나라'의 국화는 무궁화! 입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "- **정규화(normalization)** : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.\n",
    "    - 예시: U.S.A와 USA와 US는 같은 뜻이지만 각각 다른 단어로 인식될 수 있으므로 통합시켜주는 것이 좋다.\n",
    "    - 예시2: 대문자와 소문자는 둘 중 하나로 통합시켜주는 것이 좋다.\n",
    "- **정제(cleaning)** : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
    "    - 노이즈라함은, 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)을 의미하기도 하지만, 분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터라고 하기도 한다.\n",
    "    - 불용어처리도 여기에 해당한다.\n",
    "    - 등장 빈도가 너무 낮은 단어를 제외하는 기법도 있다.\n",
    "    - 길이가 너무 짧은 단어를 제외하는 기법도 있다.\n",
    "        - 영어는 단어가 보통 5\\~6글자로 되어있어서 이 기법이 효과적이라고 알려져있지만, 한글은 단어가 1\\~2글자로 이루어진 경우가 많아서 한글에는 적용하기 어렵다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state-of-the-art']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이런건 하나의 뜻을 가진 관용표현이므로 split하지 않음.\n",
    "# Normalization\n",
    "\n",
    "word_tokenize(\"state-of-the-art\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'd like to learn more somthing.             i'd like to learn more somthing.             what're i'm, isn't\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대소문자 중 하나로 통합하기\n",
    "# Normalization\n",
    "\n",
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I d like to learn more somthing              I d like to learn more somthing              what re i m  isn t'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특수문자 제거하기\n",
    "# cleaning\n",
    "\n",
    "re.sub(r\"[{}]\".format(re.escape(punctuation)),\n",
    "       \" \", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['우리나라', '의', '국화는', '무궁화', '입니다']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 특수문자 제거하기\n",
    "# cleaning\n",
    "\n",
    "re.sub(r\"[{}]\".format(re.escape(punctuation)),\n",
    "       \" \", \"'우리나라'의 국화는 무궁화! 입니다.\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state', 'of', 'the', 'art']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특수문자 제거할 때 생길 수 있는 문제 예시.\n",
    "\n",
    "re.sub(r\"[{}]\".format(re.escape(punctuation)),\n",
    "       \" \", \"state-of-the-art\").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> state-of-the-art는 그 자체가 하나의 뜻을 갖는 관용구이다. 그래서 특수문자를 기준으로 쪼개게 되면 그 뜻을 잃어버린다.\n",
    ">\n",
    "> 특수문자를 제외시킬때는 이런 경우를 염두에 두고 어디까지를 노이즈로 볼 것인지 고민해야한다.\n",
    ">\n",
    "> 일반적으로 적용할 수 있는 법칙은 없고, 데이터를 보고 상황에 따라서 맞춰야한다. \n",
    ">\n",
    "> 예를들어 이런 관용구가 많다면 \"-\"를 모두 제거하는 것은 위험할 수 있고, 별로 없다면 그냥 무시하는 것이 전체적인 성능향상에는 도움이 될 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords(불용어 처리)\n",
    "- corpus로부터, 유의미한 단어 토큰만 남기기 위해서는 무의미한 단어 토큰을 지워야한다. 무의미한 단어토큰은 자주 등장하지만 문장을 분석하는데는 크게 도움이 되지 않는 토큰을 의미한다. 이런 무의미한 단어토큰을 불용어라고 한다.\n",
    "- 예를 들어서 I, me, on, for 등의 단어들은 등장 빈도는 매우 높지만 문장을 분석하는데 결정적 역할을 하지는 않으므로 제거하는 편이 도움이 된다. \n",
    "- 불용어를 미리 정해서 모아놓는다면, 작업이 편리할 것이다. 개발자가 직접 불용어를 선정할수도 있고, NLTK에서는 패키지 내에서 불용어를 정의해두어서 쉽게 사용할 수 있다. (단, 한국어는 지원되지 않는다..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  nltk.corpus import stopwords\n",
    "\n",
    "# NLTK의 불용어 셋은 언어별로 있다.\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "i me my myself we our ours ourselves you you're you've you'll you'd\n",
      "your yours yourself yourselves he him his himself she she's her hers\n",
      "herself it it's its itself they them their theirs themselves what\n",
      "which who whom this that that'll these those am is are was were be\n",
      "been being have has had having do does did doing a an the and but if\n",
      "or because as until while of at by for with about against between into\n",
      "through during before after above below to from up down in out on off\n",
      "over under again further then once here there when where why how all\n",
      "any both each few more most other some such no nor not only own same\n",
      "so than too very s t can will just don don't should should've now d ll\n",
      "m o re ve y ain aren aren't couldn couldn't didn didn't doesn doesn't\n",
      "hadn hadn't hasn hasn't haven haven't isn isn't ma mightn mightn't\n",
      "mustn mustn't needn needn't shan shan't shouldn shouldn't wasn wasn't\n",
      "weren weren't won won't wouldn wouldn't\n"
     ]
    }
   ],
   "source": [
    "# 영어 불용어 목록\n",
    "\n",
    "print(len(stopwords.open(\"english\").read().split()))\n",
    "print(\"\\n\".join(textwrap.wrap(stopwords.open(\"english\").read())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like learn somthing like learn somthing'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 예제 문장을 불용어처리해보자.\n",
    "\n",
    "stop = stopwords.open(\"english\").read().split()\n",
    "tokens = list()\n",
    "for _ in re.sub(r\"[{}]\".format(re.escape(punctuation)),\n",
    "        \" \", sentence.lower()).split():\n",
    "    if not _ in stop:\n",
    "        tokens.append(_)\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped:  i\n",
      "Skipped:  you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 또다른 문장으로 불용어처리\n",
    "\n",
    "tokens = []\n",
    "for _ in \"I Like You\".lower().split():\n",
    "    if _ in stop:\n",
    "        print(\"Skipped: \", _)\n",
    "    else:\n",
    "        tokens.append(_)\n",
    "        \n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16945, 7944, 16821, 6972)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus로 불용어처리\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "emma = gutenberg.open(gutenberg.fileids()[0]).read().lower()\n",
    "\n",
    "removePunc = lambda t: re.sub(r\"[{}]\".format(re.escape(punctuation)),\n",
    "                             \" \", t)\n",
    "removeStop = lambda t: [_ for _ in t.split()\n",
    "                       if _ not in stop]\n",
    "\n",
    "# 각 처리결과별 Unique token의 갯수 비교\n",
    "len(set(emma.split())), len(set(word_tokenize(emma))), \\\n",
    "len(set(removeStop(emma))), \\\n",
    "len(set(removeStop(removePunc(emma))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 구두점(특수기호)를 정제(cleaning)하고, 불용어처리(stopwords)한 결과가 6972개로 unique token갯수가 가장 많이 줄어들었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('the', 5120),\n",
       "  ('to', 5079),\n",
       "  ('and', 4445),\n",
       "  ('of', 4196),\n",
       "  ('a', 3055),\n",
       "  ('i', 2602),\n",
       "  ('was', 2302),\n",
       "  ('she', 2169),\n",
       "  ('in', 2091),\n",
       "  ('not', 2028)],\n",
       " [(',', 12016),\n",
       "  ('.', 6351),\n",
       "  ('the', 5201),\n",
       "  ('to', 5181),\n",
       "  ('and', 4877),\n",
       "  ('of', 4284),\n",
       "  ('i', 3177),\n",
       "  ('a', 3124),\n",
       "  ('--', 3100),\n",
       "  ('it', 2503)],\n",
       " [('mr.', 1097),\n",
       "  ('could', 800),\n",
       "  ('would', 795),\n",
       "  ('mrs.', 675),\n",
       "  ('miss', 568),\n",
       "  ('must', 543),\n",
       "  ('emma', 481),\n",
       "  ('much', 427),\n",
       "  ('every', 425),\n",
       "  ('said', 392)],\n",
       " [('mr', 1154),\n",
       "  ('emma', 865),\n",
       "  ('could', 837),\n",
       "  ('would', 821),\n",
       "  ('mrs', 701),\n",
       "  ('miss', 602),\n",
       "  ('must', 571),\n",
       "  ('harriet', 506),\n",
       "  ('much', 486),\n",
       "  ('said', 484)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import Text\n",
    "Text(emma.split()).vocab().most_common(10), \\\n",
    "Text(word_tokenize(emma)).vocab().most_common(10),\\\n",
    "Text(removeStop(emma)).vocab().most_common(10), \\\n",
    "Text(removeStop(removePunc(emma))).vocab().most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 구두점(특수기호)를 정제(cleaning)하고, 불용어처리(stopwords)한 결과가 그냥 split 또는 word_tokenize만 한 경우보다 좀더 유의미한 token들이 남은 것으로 보인다.\n",
    ">\n",
    "> \"the\", \"to\", \".\" 등의 token보다 \"mr.\", \"emma\", \"could\"가 문장분석에 더 유의미할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# korstop (한국어 불용어 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('나 에게 너 는 사랑 이다 ', ['나', '너', '사랑'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임의의 불용어셋 만들기\n",
    "\n",
    "sentence = \"나 에게 너 는 사랑 이다.\"\n",
    "stop = \"은\\n는\\n이\\n가\\n에게\\n이다\"\n",
    "\n",
    "removePunc(sentence), \\\n",
    "removeStop(removePunc(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = kolaw.open(kolaw.fileids()[0]).read()\n",
    "# 대소문자 => 한글이니까 생략\n",
    "\n",
    "\n",
    "corpus = removePunc(corpus) # 구두점 => ' '으로 바꿈\n",
    "\n",
    "corpus = corpus.split() # 어절분리(토큰분리 - 형태소, 품사, 정규식, word_tokenize, BPE, ngram 등의 기법이 있음.)\n",
    "\n",
    "tokens1 = defaultdict(int)\n",
    "for _ in corpus:\n",
    "    tokens1[_] += 1\n",
    "\n",
    "# 단어의 길이로 정규화\n",
    "tokens2 = {t:f for t, f in tokens1.items()\n",
    "         if len(t) > 1}\n",
    "\n",
    "# 단어의 빈도로 정규화(저빈도)\n",
    "tokens3 = {t:f for t, f in tokens2.items()\n",
    "         if f > 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4180, 4180, 3859, 1846)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 단어 갯수 4180개가 1846까지 줄어들었다.\n",
    "\n",
    "len(corpus), sum(tokens1.values()), \\\n",
    "sum(tokens2.values()), sum(tokens3.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2018, 1994, 228)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유니크한 단어의 갯수도 228개 까지 줄어들었다.\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
