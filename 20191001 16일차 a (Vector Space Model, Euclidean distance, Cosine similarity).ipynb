{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model\n",
    "- Rel(D, Q) = Documentì™€ Queryì˜ ê´€ë ¨ì„±\n",
    "- Rel = Similarity\n",
    "    - vector ìƒì—ì„œëŠ” ê±°ë¦¬, ê°ë„ ë¡œ ìœ ì‚¬ë„(similarity)ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\n",
    "    - ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ ê´€ë ¨ì„±ì´ ë†’ì€ ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ì—¬, ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "\n",
    "## How to measure similarity?\n",
    "1. Euclidean distance\n",
    "2. Cosine Similarity\n",
    "\n",
    "**ì˜¤ëŠ˜ì˜ í•™ìŠµ ëª©í‘œ**: Euclidean distanceì™€ Cosine similarityë¡œ Vector space modelì—ì„œ ê²€ìƒ‰í•˜ê¸°."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linked-list ë§Œë“¤ê¸°\n",
    "\n",
    "- ì˜ˆì œ collectionìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•˜ê¸°\n",
    "- ê°„ë‹¨í•˜ê²Œ í•´ë³´ê¸° ìœ„í•´ì„œ Postingì€ python listë¥¼ ì´ìš©í•œë‹¤.\n",
    "    - Postingì€ ì €ì¥ê³µê°„ì„ ë¶„ë¦¬ì‹œì¼œì•¼í•˜ì§€ë§Œ, ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” í¸ì˜ë¥¼ ìœ„í•´ì„œ ë©”ëª¨ë¦¬ìƒì— ë‘”ë‹¤.\n",
    "    - listì˜ indexë¥¼ pointerë¡œ í™œìš©í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "_Collections = [\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"A\"],\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"B\"],\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"C\"],\n",
    "]\n",
    "\n",
    "_Vocabulary = list()\n",
    "_Lexicon = defaultdict(lambda: -1)\n",
    "_Document = defaultdict(int)\n",
    "_Posting = list()\n",
    "\n",
    "for d in _Collections:\n",
    "    _localPosting = defaultdict(int)\n",
    "    for t in d:\n",
    "        if t not in _Vocabulary:\n",
    "            _Vocabulary.append(t)\n",
    "        _localPosting[t] += 1\n",
    "    docID = len(_Document)\n",
    "    _Document[docID] = max(_localPosting.values())\n",
    "    for t, f in _localPosting.items():\n",
    "        ptr = _Lexicon[t]\n",
    "        nextPtr = len(_Posting)\n",
    "        _Posting.append((docID, f, ptr))\n",
    "        _Lexicon[t] = nextPtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF ê³„ì‚°ì‹ ì •ì˜\n",
    "\n",
    "Vector Space Modelì—ì„œëŠ” conceptì— ë”°ë¼ì„œ ë‹¨ì–´ë¥¼ ë²¡í„° ê³µê°„ì— í‘œí˜„í•œë‹¤.  \n",
    "conceptëŠ” controled wordsì™€ ê°™ì€ ëœ»ì´ë‹¤.  \n",
    "ë‹¨ì–´ 1ê°œë‹¹ ì°¨ì› 1ê°œë¥¼ ì˜ë¯¸í•œë‹¤.  \n",
    "conceptë¥¼ ì˜ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œ TF-IDF ê¸°ë²•ì„ ì´ìš©í•´ì„œ ê°€ì¤‘ì¹˜(weight)ë¡œ ì‚°ì¶œí•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "tf1 = lambda t: 1\n",
    "tf2 = lambda _struct: _struct[1]\n",
    "tf3 = lambda t:0\n",
    "tf4 = lambda _struct, t: log(1 + _struct[1])\n",
    "tf6 = lambda tf, maxtf, a:a+(1-a)*(tf/maxtf)\n",
    "tf5 = lambda tf, maxtf: tf6(tf, maxtf, 0.5)\n",
    "idf1 = lambda df, N:log(N/df) # ì¼ë°˜ì ì¸ idf\n",
    "idf2 = lambda df, N:log(N/(1+df)) \n",
    "    # the, a ë“± ë¶ˆìš©ì–´ëŠ” ëª¨ë“  docì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ log1=0ì´ ë  ìˆ˜ ìˆë‹¤.\n",
    "    # í•´ê²°: smoothing ê¸°ë²•(1ì„ ë”í•´ì¤Œ)\n",
    "idf3 = lambda df, N:log((1 + N-df)/df) \n",
    "    # N-dfê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ 1ì„ ë”í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°€ì¤‘ì¹˜ ë§¤ê¸°ê¸°\n",
    "- indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "\n",
    "# ë‘ ì  ê°„ì˜ ì°¨ì´ì— ëŒ€í•œ ì œê³±ì„ ê³„ì† summationí•  ê²ƒì´ë‹¤.\n",
    "# ë‚˜ì¤‘ì— sqrt(ì œê³±ê·¼)í•˜ë©´ ìœ í´ë¦¬ë“œ ê±°ë¦¬ê°€ ëœë‹¤.\n",
    "\n",
    "distance = lambda x1, x2: (x2 - x1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¨ì–´:A, ë¬¸ì„œ:2, ë¹ˆë„:4, ìµœê³ ë¹ˆë„:4, ê°€ì¤‘ì¹˜=-0.29\n",
      "ë‹¨ì–´:A, ë¬¸ì„œ:1, ë¹ˆë„:4, ìµœê³ ë¹ˆë„:4, ê°€ì¤‘ì¹˜=-0.29\n",
      "ë‹¨ì–´:A, ë¬¸ì„œ:0, ë¹ˆë„:5, ìµœê³ ë¹ˆë„:5, ê°€ì¤‘ì¹˜=-0.29\n",
      "ë‹¨ì–´:B, ë¬¸ì„œ:1, ë¹ˆë„:1, ìµœê³ ë¹ˆë„:4, ê°€ì¤‘ì¹˜=0.10\n",
      "ë‹¨ì–´:C, ë¬¸ì„œ:2, ë¹ˆë„:1, ìµœê³ ë¹ˆë„:4, ê°€ì¤‘ì¹˜=0.10\n"
     ]
    }
   ],
   "source": [
    "N = len(_Collections)\n",
    "_Weight = list()\n",
    "_WLexicon = defaultdict(lambda:{\"Posting\":None, \"DF\":0})\n",
    "_DocLength = defaultdict(float)\n",
    "for t, ptr in _Lexicon.items():\n",
    "    dfPtr = ptr\n",
    "    df = 0\n",
    "    while dfPtr != -1:\n",
    "        _struct = _Posting[dfPtr]\n",
    "        df += 1\n",
    "        dfPtr = _struct[-1]\n",
    "    \n",
    "    wptr = len(_Weight)\n",
    "    while ptr != -1:\n",
    "        _struct = _Posting[ptr]\n",
    "        tf = _struct[1]\n",
    "        maxtf = _Document[_struct[0]]\n",
    "        w = tf6(tf, maxtf, 0)* idf2(df, N)\n",
    "        print(\"ë‹¨ì–´:{0}, ë¬¸ì„œ:{1}, ë¹ˆë„:{2}, ìµœê³ ë¹ˆë„:{3}, ê°€ì¤‘ì¹˜={4:.2f}\"\n",
    "              .format(\n",
    "                  t, _struct[0], _struct[1],\n",
    "                  maxtf, w))\n",
    "        \n",
    "        ptr = _struct[-1]\n",
    "        \n",
    "        wStruct = (_struct[0], w)\n",
    "        _Weight.append(wStruct)\n",
    "        _DocLength[_struct[0]] += distance(0, w) # Cosine similarity ê³„ì‚°ìš©\n",
    "    _WLexicon[t][\"Posting\"] = wptr\n",
    "    _WLexicon[t][\"DF\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query ê²€ìƒ‰í•˜ê¸° (Euclidean distance ì´ìš©)\n",
    "- query parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "Q = \"B B B\"\n",
    "result = defaultdict(float)\n",
    "QueryRepr = defaultdict(int) # localPostingê°€ ê°™ì€ ì—­í• ; freqë¥¼ ì„¼ë‹¤.\n",
    "QueryWeight = defaultdict(float)\n",
    "for t in Q.split():\n",
    "    if t in _Vocabulary:  # _Vocabularyì— ìˆëŠ” ë‹¨ì–´ë§Œ ì¿¼ë¦¬ë¡œ ì‚¬ìš©\n",
    "        QueryRepr[t] += 1\n",
    "maxQuery = max(QueryRepr.values())\n",
    "for t, f in QueryRepr.items():\n",
    "    w = tf6(f, maxQuery, 0)*idf2(_WLexicon[t][\"DF\"], N)\n",
    "    QueryWeight[t] = w\n",
    "\n",
    "for t in _Vocabulary:\n",
    "    ptr = _WLexicon[t][\"Posting\"]\n",
    "    df = _WLexicon[t][\"DF\"]\n",
    "    for _struct in _Weight[ptr:ptr+df]:\n",
    "        _struct[0] # => ë¬¸ì„œ\n",
    "        _struct[1] # => ê°€ì¤‘ì¹˜\n",
    "        result[_struct[0]] += distance(QueryWeight[t],\n",
    "                                      _struct[1])\n",
    "    \n",
    "result = {d:sqrt(dist)  for d, dist in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.3050181911435358, 1: 0.41861327484332994, 0: 0.2876820724517809}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"B B B\"ë¼ëŠ” queryì— ëŒ€í•˜ì—¬ Bê°€ ë“¤ì–´ìˆëŠ” ë‘ë²ˆì§¸ ë¬¸ì„œê°€ ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ë³´ì¸ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query ê²€ìƒ‰í•˜ê¸° (Cosine similarity ì´ìš©)\n",
    "- query parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = \"A A A A B\"\n",
    "result = defaultdict(float)\n",
    "QueryRepr = defaultdict(int)\n",
    "QueryWeight = defaultdict(float)\n",
    "QueryLength = 0.0\n",
    "for t in Q.split():\n",
    "    if t in _Vocabulary:\n",
    "        QueryRepr[t] += 1\n",
    "maxQuery = max(QueryRepr.values())\n",
    "for t, f in QueryRepr.items():\n",
    "    w = tf6(f, maxQuery, 0)*idf2(_WLexicon[t][\"DF\"], N)\n",
    "    QueryWeight[t]= w\n",
    "    QueryLength += distance(0,w)\n",
    "\n",
    "for t in QueryRepr:\n",
    "    ptr = _WLexicon[t][\"Posting\"]\n",
    "    df = _WLexicon[t][\"DF\"]\n",
    "    for _struct in _Weight[ptr:ptr+df]:\n",
    "        _struct[0] # => ë¬¸ì„œ\n",
    "        _struct[1] # => ê°€ì¤‘ì¹˜\n",
    "        \n",
    "        # resultë¥¼ ê³„ì‚°í•  ë•Œ cosine similarity ìˆ˜ì‹ì„ ì´ìš©í•œë‹¤.\n",
    "        result[_struct[0]] += QueryWeight[t] * _struct[1]    \n",
    "result = {d:ip/(sqrt(QueryLength)*sqrt(_DocLength[d]))\n",
    "          for d, ip in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {2: 0.09303609692847455,\n",
       "             1: 0.09303609692847455,\n",
       "             0: 0.08276097481015171})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_DocLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.889557682904279, 1: 1.0000000000000002, 0: 0.9431636564797644}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Queryì™€ ë‘ë²ˆì§¸ documentê°€ ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ìœ ì‚¬ë„ê°€ 1ë¡œ ë‚˜ì™”ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postingì„ localì— ì €ì¥í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ êµ¬í˜„í•˜ê¸°\n",
    "- ìœ„ì™€ ë™ì¼í•œ ë°©ë²•ì´ë‚˜, Positingì„ localì— ì €ì¥í•˜ì—¬ linked-listë¥¼ ì¢€ ë” ì›ì¹™ì ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ì‹¤ìŠµ.\n",
    "- python openì˜ tellê³¼ seek ë©”ì„œë“œë¥¼ pointerë¡œ í™œìš©í•¨.\n",
    "- ìœ„ì˜ ì‹¤ìŠµì—ì„œ ì½”ë“œê°€ ë°”ë€ ë¶€ë¶„ì€ ì£¼ì„ì²˜ë¦¬í•˜ì—¬ ì°¨ì´ë¥¼ ì•Œì•„ë³´ê¸° ì‰½ê²Œ í•˜ì˜€ìŒ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struct import pack, unpack\n",
    "\n",
    "_Collections = [\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"A\"],\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"B\"],\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"C\"],\n",
    "]\n",
    "\n",
    "_Vocabulary = list()\n",
    "_Lexicon = defaultdict(lambda: -1)\n",
    "_Document = defaultdict(int)\n",
    "# _Posting = list()\n",
    "\n",
    "fp = open(\"posting.dat\", \"wb\")\n",
    "\n",
    "for d in _Collections:\n",
    "    _localPosting = defaultdict(int)\n",
    "    for t in d:\n",
    "        if t not in _Vocabulary:\n",
    "            _Vocabulary.append(t)\n",
    "        _localPosting[t] += 1\n",
    "    docID = len(_Document)\n",
    "    _Document[docID] = max(_localPosting.values())\n",
    "    for t, f in _localPosting.items():\n",
    "        ptr = _Lexicon[t]\n",
    "#         nextPtr = len(_Posting)\n",
    "#         _Posting.append((docID, f, ptr))\n",
    "        nextPtr = fp.tell()\n",
    "        fp.write(pack(\"iii\", docID, f, ptr))\n",
    "        _Lexicon[t] = nextPtr\n",
    "        \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°€ì¤‘ì¹˜ ë§¤ê¸°ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¨ì–´:A, ë¬¸ì„œ:2, ë¹ˆë„:4, ìµœê³ ë¹ˆë„:4,         ê°€ì¤‘ì¹˜=-0.29\n",
      "ë‹¨ì–´:A, ë¬¸ì„œ:1, ë¹ˆë„:4, ìµœê³ ë¹ˆë„:4,         ê°€ì¤‘ì¹˜=-0.29\n",
      "ë‹¨ì–´:A, ë¬¸ì„œ:0, ë¹ˆë„:5, ìµœê³ ë¹ˆë„:5,         ê°€ì¤‘ì¹˜=-0.29\n",
      "ë‹¨ì–´:B, ë¬¸ì„œ:1, ë¹ˆë„:1, ìµœê³ ë¹ˆë„:4,         ê°€ì¤‘ì¹˜=0.10\n",
      "ë‹¨ì–´:C, ë¬¸ì„œ:2, ë¹ˆë„:1, ìµœê³ ë¹ˆë„:4,         ê°€ì¤‘ì¹˜=0.10\n"
     ]
    }
   ],
   "source": [
    "N = len(_Collections)\n",
    "_Weight = list()\n",
    "_WLexicon = defaultdict(lambda:{\"Posting\":None, \"DF\":0})\n",
    "_DocLength = defaultdict(float)\n",
    "\n",
    "fp = open(\"posting.dat\", \"rb\")\n",
    "wp = open(\"weight.dat\", \"wb\")\n",
    "\n",
    "for t, ptr in _Lexicon.items():\n",
    "    dfPtr = ptr\n",
    "    df = 0\n",
    "    while dfPtr != -1:\n",
    "#         _struct = _Posting[dfPtr]\n",
    "        fp.seek(dfPtr)\n",
    "        _struct = unpack(\"iii\", fp.read(4*3))\n",
    "        df += 1\n",
    "        dfPtr = _struct[-1]\n",
    "        \n",
    "#     wptr = len(_Weight)\n",
    "    wptr = wp.tell()\n",
    "    while ptr != -1:\n",
    "#         _struct = _Posting[ptr]\n",
    "        fp.seek(ptr)\n",
    "        _struct = unpack(\"iii\", fp.read(4*3))\n",
    "        tf = _struct[1]\n",
    "        maxtf = _Document[_struct[0]]\n",
    "        w = tf6(tf, maxtf, 0)* idf2(df, N)\n",
    "        print(\"ë‹¨ì–´:{0}, ë¬¸ì„œ:{1}, ë¹ˆë„:{2}, ìµœê³ ë¹ˆë„:{3}, \\\n",
    "        ê°€ì¤‘ì¹˜={4:.2f}\"\n",
    "              .format(\n",
    "                  t, _struct[0], _struct[1],\n",
    "                  maxtf, w))\n",
    "        ptr = _struct[-1]\n",
    "        \n",
    "#         wStruct = (_struct[0], w)\n",
    "#         _Weight.append(wStruct)\n",
    "        wp.write(pack(\"if\", _struct[0], w))\n",
    "        _DocLength[_struct[0]] += distance(0, w)\n",
    "    _WLexicon[t][\"Posting\"] = wptr\n",
    "    _WLexicon[t][\"DF\"] = df\n",
    "    \n",
    "fp.close()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query ê²€ìƒ‰í•˜ê¸° (Euclidean distance ì´ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "Q = \"B B B\"\n",
    "result = defaultdict(float)\n",
    "QueryRepr = defaultdict(int) \n",
    "QueryWeight = defaultdict(float)\n",
    "for t in Q.split():\n",
    "    if t in _Vocabulary:  \n",
    "        QueryRepr[t] += 1\n",
    "maxQuery = max(QueryRepr.values())\n",
    "for t, f in QueryRepr.items():\n",
    "    w = tf6(f, maxQuery, 0)*idf2(_WLexicon[t][\"DF\"], N)\n",
    "    QueryWeight[t] = w\n",
    "\n",
    "wp = open(\"weight.dat\", \"rb\")\n",
    "    \n",
    "for t in _Vocabulary:\n",
    "    ptr = _WLexicon[t][\"Posting\"]\n",
    "    df = _WLexicon[t][\"DF\"]\n",
    "#     for _struct in _Weight[ptr:ptr+df]:\n",
    "    wp.seek(ptr)\n",
    "    for i in range(df):\n",
    "        _struct = unpack(\"if\", wp.read(4*2))\n",
    "        result[_struct[0]] += distance(QueryWeight[t],\n",
    "                                      _struct[1])\n",
    "\n",
    "wp.close()\n",
    "result = {d:sqrt(dist)  for d, dist in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query ê²€ìƒ‰í•˜ê¸° (Cosine similarity ì´ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "Q = \"A A A A B\"\n",
    "result = defaultdict(float)\n",
    "QueryRepr = defaultdict(int)\n",
    "QueryWeight = defaultdict(float)\n",
    "QueryLength = 0.0\n",
    "for t in Q.split():\n",
    "    if t in _Vocabulary:\n",
    "        QueryRepr[t] += 1\n",
    "maxQuery = max(QueryRepr.values())\n",
    "fp = open(\"weight.dat\", \"rb\")\n",
    "for t, f in QueryRepr.items():\n",
    "    w = tf6(f, maxQuery, 0)*idf2(_WLexicon[t][\"DF\"], N)\n",
    "    QueryWeight[t]= w\n",
    "    QueryLength += distance(0,w)\n",
    "\n",
    "for t in QueryRepr:\n",
    "    ptr = _WLexicon[t][\"Posting\"]\n",
    "    df = _WLexicon[t][\"DF\"]\n",
    "    fp.seek(ptr)\n",
    "    for i in range(df):\n",
    "        _struct = unpack(\"if\", fp.read(4*2))\n",
    "        _struct[0] # => ë¬¸ì„œ\n",
    "        _struct[1] # => ê°€ì¤‘ì¹˜\n",
    "        result[_struct[0]] += QueryWeight[t] * _struct[1]  \n",
    "result = {d:ip/(sqrt(QueryLength)*sqrt(_DocLength[d]))\n",
    "          for d, ip in result.items()}\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.8895577255065847, 1: 1.0000000393683177, 0: 0.9431637016493439}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•œê¸€ corpusë¡œ ì‹¤ì „ ì—°ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JINHYO\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.corpus import kobill\n",
    "from konlpy.tag import Kkma\n",
    "from struct import pack, unpack\n",
    "\n",
    "\n",
    "Lexicon = defaultdict(lambda:{\"ptr\":-1, \"df\":0})\n",
    "# ë‹¨ì–´:{ìœ„ì¹˜, ì´ëª‡ê°œì˜ë¬¸ì„œ}\n",
    "\n",
    "Documents = defaultdict(lambda:{\n",
    "    \"length\":0.0, \"ttf\":0, \"max\":0})\n",
    "# ë¬¸ì„œ:(ë¬¸ì„œë²¡í„°ì˜ê¸¸ì´, ì´ëª‡ê°œì˜ë‹¨ì–´, ì´ë¬¸ì„œì—ì„œê°€ì¥ë§ì´ë‚˜ì˜¨ë‹¨ì–´ì˜ë¹ˆë„)\n",
    "# ttf = total term frequency\n",
    "\n",
    "DocumentsList = list()\n",
    "# ì¸ë±ìŠ¤:ë¬¸ì„œì˜ì œëª©\n",
    "\n",
    "kkma = Kkma()\n",
    "# í˜•íƒœì†Œë¶„ì„ê¸°(ê¼¬ê¼¬ë§ˆ)\n",
    "\n",
    "fp = open(\"inverted.dat\", \"wb\")\n",
    "# TDM(frequency)\n",
    "\n",
    "for docName in kobill.fileids():\n",
    "    # Local\n",
    "    document = kobill.open(docName).read() # ê°œë³„ë¬¸ì„œ\n",
    "    localPostings = defaultdict(int) # ê° ë¬¸ì„œì˜ TDM(Vector)\n",
    "    # ë¬¸ì„œì •ë³´ ì €ì¥\n",
    "    DocID = len(DocumentsList) # ë¬¸ì„œì˜ ì œëª© -> ìˆ«ì\n",
    "    DocumentsList.append(docName) # í•´ë‹¹ ìˆ«ì(ìœ„ì¹˜)  ì œëª© ì €ì¥\n",
    "    # ê° ë¬¸ì„œì—ì„œ ìƒ‰ì¸ì–´ ì¶”ì¶œ ë°©ì‹ (í˜•íƒœì†Œë¶„ì„ê¸°, í˜•íƒœì†Œì˜ê¸¸ì´ë¡œì •ê·œí™”)\n",
    "    for term in [_ for _ in kkma.morphs(document)\n",
    "              if 1 < len(_) < 6]:\n",
    "        localPostings[term] += 1\n",
    "        # ë¬¸ì„œ ë²¡í„° ìƒì„± (ì—´ ë‹¨ì–´:ë¹ˆë„)\n",
    "    # ë¬¸ì„œì˜ í†µê³„ì •ë³´ ì €ì¥ => for weight\n",
    "    Documents[DocID][\"ttf\"] = sum(localPostings.values())\n",
    "    Documents[DocID][\"max\"] = max(localPostings.values())\n",
    "    # Global\n",
    "    for term, freq in localPostings.items():\n",
    "            if term not in Lexicon:\n",
    "        ptr = Lexicon[term][\"ptr\"]\n",
    "        # 1. ë‹¨ì–´ê°€ ì²« ë“±ì¥: ìœ„ì¹˜ ptr=-1\n",
    "        # 2. ë‹¨ì–´ê°€ ìˆì„ ë•Œ, ptr= ë§ˆì§€ë§‰ ì €ì¥ ìœ„ì¹˜\n",
    "        postingPtr= fp.tell()\n",
    "        # íŒŒì¼ì˜ ì–´ëŠ ìœ„ì¹˜ì— ì €ì¥í•˜ëŠ” ì§€ \n",
    "        fp.write(pack(\"iii\", DocID, freq, ptr))\n",
    "        # êµ¬ì¡°ì²´ë¥¼ ì €ì¥(int, int, int)\n",
    "        Lexicon[term][\"ptr\"] = postingPtr\n",
    "        # 1.1 ìœ„ì¹˜ ë³€ê²½ : íŒŒì¼ì˜ ìœ„ì¹˜\n",
    "        Lexicon[term][\"df\"] += 1\n",
    "        # for IDF\n",
    "        \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {0: {'length': 0.0, 'ttf': 883, 'max': 123},\n",
       "             1: {'length': 0.0, 'ttf': 883, 'max': 119},\n",
       "             2: {'length': 0.0, 'ttf': 1039, 'max': 200},\n",
       "             3: {'length': 0.0, 'ttf': 893, 'max': 92},\n",
       "             4: {'length': 0.0, 'ttf': 213, 'max': 11},\n",
       "             5: {'length': 0.0, 'ttf': 346, 'max': 35},\n",
       "             6: {'length': 0.0, 'ttf': 1803, 'max': 122},\n",
       "             7: {'length': 0.0, 'ttf': 672, 'max': 35},\n",
       "             8: {'length': 0.0, 'ttf': 642, 'max': 31},\n",
       "             9: {'length': 0.0, 'ttf': 1855, 'max': 430}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexer => Weighting(weight.dat)\n",
    "\n",
    "from math import log2\n",
    "\n",
    "fp1 = open(\"inverted.dat\", \"rb\")\n",
    "fp2 = open(\"weight.dat\", \"wb\")\n",
    "\n",
    "N = len(DocumentsList)\n",
    "for term, termStruct in Lexicon.items():\n",
    "    ptr = termStruct[\"ptr\"]\n",
    "    wPtr = fp2.tell()\n",
    "    while ptr != -1:\n",
    "        fp1.seek(ptr)\n",
    "        _struct = unpack(\"iii\", fp1.read(4*3)) # 12byte\n",
    "        _struct[0] # => ë¬¸ì„œ ID\n",
    "        _struct[1] # => í•´ë‹¹ ë¬¸ì„œì—ì„œì˜ ë¹ˆë„(tf)\n",
    "        TF = _struct[1] / Documents[_struct[0]][\"max\"]\n",
    "        IDF = log2(N/termStruct[\"df\"])\n",
    "        Documents[_struct[0]][\"length\"] += TF*IDF\n",
    "        fp2.write(pack(\"if\", _struct[0], TF*IDF))\n",
    "        ptr = _struct[-1]\n",
    "    Lexicon[term][\"ptr\"] = wPtr\n",
    "fp2.close()\n",
    "fp1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Parser => QyeryRepr(with Weight)\n",
    "\n",
    "Query = \"í˜„í–‰ë²•ì€ ì…ë²•ì˜ˆê³ ì™€ í–‰ì •ì˜ˆê³ ë¥¼ í†µí•˜ì—¬ ì •ì±… ê²°ì • ê³¼ì •ì—\"\n",
    "\n",
    "localPostings = defaultdict(int)\n",
    "\n",
    "for term in [_ for _ in kkma.morphs(Query)\n",
    "                if 1 < len(_) < 6]:\n",
    "    localPostings[term] += 1\n",
    "maxTF = max(localPostings.values())\n",
    "# TTF = sum(localPostings.values()) => ë‚˜ì¤‘ì— BM25ì—ì„œ ì“¸ê±°ë‹¤.\n",
    "for term, freq, in localPostings.items():\n",
    "    TF = freq/maxTF\n",
    "    IDF = log2(N/Lexicon[term][\"df\"])\n",
    "    localPostings[term] = TF*IDF\n",
    "    \n",
    "# ==> Query Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'í˜„í–‰ë²•': 0.5,\n",
       "             'ì…ë²•': 1.660964047443681,\n",
       "             'ì˜ˆê³ ': 3.321928094887362,\n",
       "             'í–‰ì •': 0.3684827970831031,\n",
       "             'í†µí•˜': 1.160964047443681,\n",
       "             'ì •ì±…': 0.2572865864148791,\n",
       "             'ê²°ì •': 1.660964047443681,\n",
       "             'ê³¼ì •': 0.6609640474436812})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localPostings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Euclidean distance\n",
    "- ì‹:\n",
    "$$\n",
    "dist(q, d) = \\sqrt{\\sum_{t\\in V}{[tf(t,q) \\cdot idf(t) - tf(t,d) \\cdot idf(t)]^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- í•´ì„: (ì‹œê·¸ë§ˆ t ã…Œ V) sqrt((Dë²¡í„°të²ˆì§¸ê°’-Që²¡í„°të²ˆì§¸ê°’)\\*\\*2)\n",
    "- ë¬¸ì œ: summationì„ í• ë•Œ, vocabularyì— ì†í•˜ëŠ” ëª¨ë“  t(ë‹¨ì–´)ì— ëŒ€í•´ì„œ ì‹¤í–‰í•˜ëŠ”ë° tëŠ” queryì— ì—†ëŠ” ë‹¨ì–´ì¼ìˆ˜ë„ ìˆë‹¤. ì´ë•Œ penaltyê°€ ë°œìƒí•œë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1809892.txt': 0.9785681821197749}\n",
      "{'1809892.txt': {'ê±°ë¦¬': 0.9785681821197749, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "êµìœ¡ê³µë¬´ì›ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ì •ì˜í™”ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "\n",
      "{'1809890.txt': 1.1213888599844724}\n",
      "{'1809890.txt': {'ê±°ë¦¬': 1.1213888599844724, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "ì§€ë°©ê³µë¬´ì›ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ì •ì˜í™”ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "\n",
      "{'1809891.txt': 1.1454561206043383}\n",
      "{'1809891.txt': {'ê±°ë¦¬': 1.1454561206043383, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "êµ­ê°€ê³µë¬´ì›ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ì •ì˜í™”ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "\n",
      "{'1809893.txt': 1.3363030928544017}\n",
      "{'1809893.txt': {'ê±°ë¦¬': 1.3363030928544017, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "ë‚¨ë…€ê³ ìš©í‰ë“±ê³¼ ì¼ Â·ê°€ì • ì–‘ë¦½ ì§€ì›ì— ê´€í•œ ë²•ë¥  \n",
      "\n",
      "ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "{'1809899.txt': 1.5382427458729653}\n",
      "{'1809899.txt': {'ê±°ë¦¬': 1.5382427458729653, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "ê²°í˜¼ì¤‘ê°œì—…ì˜ ê´€ë¦¬ì— ê´€í•œ ë²•ë¥  ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(í•œì„ êµì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "\n",
      "{'1809895.txt': 2.3306777198717277}\n",
      "{'1809895.txt': {'ê±°ë¦¬': 2.3306777198717277, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "í•˜ë„ê¸‰ê±°ë˜ ê³µì •í™”ì— ê´€í•œ ë²•ë¥  ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ìœ ì„ í˜¸ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "\n",
      "{'1809896.txt': 3.0498591928696674}\n",
      "{'1809896.txt': {'ê±°ë¦¬': 3.0498591928696674, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "í–‰ì •ì ˆì°¨ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ìœ ì„ í˜¸ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "\n",
      "{'1809898.txt': 4.807353198923258}\n",
      "{'1809898.txt': {'ê±°ë¦¬': 4.807353198923258, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "êµ­êµ°ë¶€ëŒ€ì˜ ì†Œë§ë¦¬ì•„ í•´ì—­ íŒŒê²¬ì—°ì¥ ë™ì˜ì•ˆ\n",
      "\n",
      "ì˜ì•ˆ\n",
      "\n",
      "{'1809897.txt': 5.050448921893771}\n",
      "{'1809897.txt': {'ê±°ë¦¬': 5.050448921893771, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "êµ­êµ°ë¶€ëŒ€ì˜ ì•„ëì—ë¯¸ë¦¬íŠ¸(UAE)êµ° êµìœ¡í›ˆë ¨ ì§€ì› ë“±ì— \n",
      "ê´€í•œ íŒŒê²¬ ë™ì˜ì•ˆ\n",
      "\n",
      "\n",
      "{'1809894.txt': 6.478089528366782}\n",
      "{'1809894.txt': {'ê±°ë¦¬': 6.478089528366782, 'ë‹¨ì–´ìˆ˜': 0}}\n",
      "ê³ ë“±êµìœ¡ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ì•ˆìƒìˆ˜ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "searchResult = defaultdict(float)\n",
    "fp = open(\"weight.dat\", \"rb\")\n",
    "for term, termStruct in Lexicon.items():\n",
    "    ptr = termStruct[\"ptr\"]\n",
    "    fp.seek(ptr)\n",
    "    for _ in range(termStruct[\"df\"]):\n",
    "        _struct = unpack(\"if\", fp.read(4*2))\n",
    "        qw = localPostings[term] # ì´ë¦„ì€ ë‚˜ì¤‘ì— ë°”ê¾¸ì‹œêµ¬ì—¬\n",
    "        dw = _struct[1]\n",
    "        searchResult[_struct[0]] += (qw-dw)**2\n",
    "fp.close()\n",
    "\n",
    "searchResult = {d:sqrt(dist)\n",
    "               for d, dist in searchResult.items()}\n",
    "\n",
    "for d, dist in {DocumentsList[_[0]]:_[1]\n",
    "                for _ in sorted(searchResult.items(), \n",
    "                        key=lambda r:r[1])}.items():\n",
    "    print({d:dist})\n",
    "    print({d:{\"ê±°ë¦¬\":dist, \"ë‹¨ì–´ìˆ˜\": Documents[d][\"ttf\"]}})\n",
    "    print(\"\\n\".join(kobill.open(d).read().splitlines()[:3]))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- euclidean distanceì™€ ë‹¤ë¥´ê²Œ summationí• ë•Œ ë‹¤ë¥´ë‹¤. \n",
    "    - ğ‘¡âˆˆğ‘âˆ©ğ‘‘\n",
    "\n",
    "- svmì€ l1, l2ë¥¼ ê³ ë¥¼ í•„ìš”ê°€ ì—†ë‹¤.\n",
    "\n",
    "- ë³´í†µ ê°œë°œí•  ë•Œ ìˆ˜ì§‘/í•™ìŠµ ì„œë²„ 1ëŒ€, ì„œë¹„ìŠ¤ ì œê³µ ì„œë²„ 1ëŒ€ í•´ì„œ ì´ 2ëŒ€ë¥¼ ì“´ë‹¤. (ì‹¤ì‹œê°„ì²˜ëŸ¼ ëŠë¼ê²Œ í•œë‹¤.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: {'ê°ë„': 7.431129331674336, 'ë‹¨ì–´ìˆ˜': 1803}}\n",
      "í–‰ì •ì ˆì°¨ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ìœ ì„ í˜¸ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "{4: {'ê°ë„': 0.1718470144736527, 'ë‹¨ì–´ìˆ˜': 213}}\n",
      "ê³ ë“±êµìœ¡ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ì•ˆìƒìˆ˜ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "{8: {'ê°ë„': 0.04033460946903825, 'ë‹¨ì–´ìˆ˜': 642}}\n",
      "êµ­êµ°ë¶€ëŒ€ì˜ ì†Œë§ë¦¬ì•„ í•´ì—­ íŒŒê²¬ì—°ì¥ ë™ì˜ì•ˆ\n",
      "\n",
      "ì˜ì•ˆ\n",
      "{7: {'ê°ë„': 0.0375486104695337, 'ë‹¨ì–´ìˆ˜': 672}}\n",
      "êµ­êµ°ë¶€ëŒ€ì˜ ì•„ëì—ë¯¸ë¦¬íŠ¸(UAE)êµ° êµìœ¡í›ˆë ¨ ì§€ì› ë“±ì— \n",
      "ê´€í•œ íŒŒê²¬ ë™ì˜ì•ˆ\n",
      "\n",
      "{3: {'ê°ë„': 0.02805257664723038, 'ë‹¨ì–´ìˆ˜': 893}}\n",
      "ë‚¨ë…€ê³ ìš©í‰ë“±ê³¼ ì¼ Â·ê°€ì • ì–‘ë¦½ ì§€ì›ì— ê´€í•œ ë²•ë¥  \n",
      "\n",
      "ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "{1: {'ê°ë„': 0.01849454212767377, 'ë‹¨ì–´ìˆ˜': 883}}\n",
      "êµ­ê°€ê³µë¬´ì›ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ì •ì˜í™”ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "{0: {'ê°ë„': 0.018212451826866254, 'ë‹¨ì–´ìˆ˜': 883}}\n",
      "ì§€ë°©ê³µë¬´ì›ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ì •ì˜í™”ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "{2: {'ê°ë„': 0.012825953697034324, 'ë‹¨ì–´ìˆ˜': 1039}}\n",
      "êµìœ¡ê³µë¬´ì›ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ì •ì˜í™”ì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "{9: {'ê°ë„': 0.012368922287890492, 'ë‹¨ì–´ìˆ˜': 1855}}\n",
      "ê²°í˜¼ì¤‘ê°œì—…ì˜ ê´€ë¦¬ì— ê´€í•œ ë²•ë¥  ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(í•œì„ êµì˜ì› ëŒ€í‘œë°œì˜ )\n",
      "{5: {'ê°ë„': 0.0, 'ë‹¨ì–´ìˆ˜': 346}}\n",
      "í•˜ë„ê¸‰ê±°ë˜ ê³µì •í™”ì— ê´€í•œ ë²•ë¥  ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ\n",
      "\n",
      "(ìœ ì„ í˜¸ì˜ì› ëŒ€í‘œë°œì˜ )\n"
     ]
    }
   ],
   "source": [
    "# Relevance(Euclidean, Cosine Theta)\n",
    "# 2. Cosine Theta\n",
    "searchResult = defaultdict(float)\n",
    "fp = open(\"weight.dat\", \"rb\")\n",
    "queryLength = 0.0\n",
    "for term, qw in localPostings.items():\n",
    "    ptr = Lexicon[term][\"ptr\"]\n",
    "    fp.seek(ptr)\n",
    "    for _ in range(Lexicon[term][\"df\"]):\n",
    "        _struct = unpack(\"if\", fp.read(4*2))\n",
    "        dw = _struct[1]\n",
    "        searchResult[_struct[0]] += (qw*dw)\n",
    "    queryLength += qw**2\n",
    "fp.close()\n",
    "\n",
    "searchResult = {d:angle/sqrt(Documents[d][\"length\"])\\\n",
    "                            *sqrt(queryLength) # ë¹¼ë„ ë¬´ë°©í•˜ì§€ë§Œ ë¹¼ë©´ ê°’ì´ 1ì„ ë„˜ì–´ê°\n",
    "               for d, angle in searchResult.items()}\n",
    "\n",
    "for d, dist in {_[0]:_[1]\n",
    "                for _ in sorted(searchResult.items(), \n",
    "                                key=lambda r:r[1], \n",
    "                                reverse=True)}.items():\n",
    "    print({d:{\"ê°ë„\":dist, \"ë‹¨ì–´ìˆ˜\": Documents[d][\"ttf\"]}})\n",
    "    print(\"\\n\".join(kobill.open(DocumentsList[d]).read().\\\n",
    "                     splitlines()[:3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ptr': 6168, 'df': 5}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon[\"í˜„í–‰ë²•\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
