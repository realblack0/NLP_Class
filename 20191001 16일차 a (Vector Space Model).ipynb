{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model\n",
    "- Rel(D, Q) = Document와 Query의 관련성\n",
    "- Rel = Similarity\n",
    "    - vector 상에서는 거리, 각도 로 유사도(similarity)를 구할 수 있다.\n",
    "    - 유사도가 높으면 관련성이 높은 것으로 판단하여, 검색 결과를 만들 수 있다.\n",
    "\n",
    "\n",
    "## How to measure similarity?\n",
    "1. Euclidean distance\n",
    "2. Cosine Similarity\n",
    "\n",
    "**오늘의 학습 목표**: Euclidean distance와 Cosine similarity로 Vector space model에서 검색하기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linked-list 만들기\n",
    "\n",
    "- 예제 collection으로 간단하게 구현하기\n",
    "- 간단하게 해보기 위해서 Posting은 python list를 이용한다.\n",
    "    - Posting은 저장공간을 분리시켜야하지만, 이번 실습에서는 편의를 위해서 메모리상에 둔다.\n",
    "    - list의 index를 pointer로 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "_Collections = [\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"A\"],\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"B\"],\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"C\"],\n",
    "]\n",
    "\n",
    "_Vocabulary = list()\n",
    "_Lexicon = defaultdict(lambda: -1)\n",
    "_Document = defaultdict(int)\n",
    "_Posting = list()\n",
    "\n",
    "for d in _Collections:\n",
    "    _localPosting = defaultdict(int)\n",
    "    for t in d:\n",
    "        if t not in _Vocabulary:\n",
    "            _Vocabulary.append(t)\n",
    "        _localPosting[t] += 1\n",
    "    docID = len(_Document)\n",
    "    _Document[docID] = max(_localPosting.values())\n",
    "    for t, f in _localPosting.items():\n",
    "        ptr = _Lexicon[t]\n",
    "        nextPtr = len(_Posting)\n",
    "        _Posting.append((docID, f, ptr))\n",
    "        _Lexicon[t] = nextPtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 계산식 정의\n",
    "\n",
    "Vector Space Model에서는 concept에 따라서 단어를 벡터 공간에 표현한다.  \n",
    "concept는 controled words와 같은 뜻이다.  \n",
    "단어 1개당 차원 1개를 의미한다.  \n",
    "concept를 잘 표현하기 위해서 TF-IDF 기법을 이용해서 가중치(weight)로 산출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "tf1 = lambda t: 1\n",
    "tf2 = lambda _struct: _struct[1]\n",
    "tf3 = lambda t:0\n",
    "tf4 = lambda _struct, t: log(1 + _struct[1])\n",
    "tf6 = lambda tf, maxtf, a:a+(1-a)*(tf/maxtf)\n",
    "tf5 = lambda tf, maxtf: tf6(tf, maxtf, 0.5)\n",
    "idf1 = lambda df, N:log(N/df) # 일반적인 idf\n",
    "idf2 = lambda df, N:log(N/(1+df)) \n",
    "    # the, a 등 불용어는 모든 doc에서 나올 수 있으므로 log1=0이 될 수 있다.\n",
    "    # 해결: smoothing 기법(1을 더해줌)\n",
    "idf3 = lambda df, N:log((1 + N-df)/df) \n",
    "    # N-df가 0이 되는 것을 방지하기 위해서 1을 더함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 매기기\n",
    "- indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "\n",
    "# 두 점 간의 차이에 대한 제곱을 계속 summation할 것이다.\n",
    "# 나중에 sqrt(제곱근)하면 유클리드 거리가 된다.\n",
    "\n",
    "distance = lambda x1, x2: (x2 - x1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어:A, 문서:2, 빈도:4, 최고빈도:4, 가중치=-0.29\n",
      "단어:A, 문서:1, 빈도:4, 최고빈도:4, 가중치=-0.29\n",
      "단어:A, 문서:0, 빈도:5, 최고빈도:5, 가중치=-0.29\n",
      "단어:B, 문서:1, 빈도:1, 최고빈도:4, 가중치=0.10\n",
      "단어:C, 문서:2, 빈도:1, 최고빈도:4, 가중치=0.10\n"
     ]
    }
   ],
   "source": [
    "N = len(_Collections)\n",
    "_Weight = list()\n",
    "_WLexicon = defaultdict(lambda:{\"Posting\":None, \"DF\":0})\n",
    "_DocLength = defaultdict(float)\n",
    "for t, ptr in _Lexicon.items():\n",
    "    dfPtr = ptr\n",
    "    df = 0\n",
    "    while dfPtr != -1:\n",
    "        _struct = _Posting[dfPtr]\n",
    "        df += 1\n",
    "        dfPtr = _struct[-1]\n",
    "    \n",
    "    wptr = len(_Weight)\n",
    "    while ptr != -1:\n",
    "        _struct = _Posting[ptr]\n",
    "        tf = _struct[1]\n",
    "        maxtf = _Document[_struct[0]]\n",
    "        w = tf6(tf, maxtf, 0)* idf2(df, N)\n",
    "        print(\"단어:{0}, 문서:{1}, 빈도:{2}, 최고빈도:{3}, 가중치={4:.2f}\"\n",
    "              .format(\n",
    "                  t, _struct[0], _struct[1],\n",
    "                  maxtf, w))\n",
    "        \n",
    "        ptr = _struct[-1]\n",
    "        \n",
    "        wStruct = (_struct[0], w)\n",
    "        _Weight.append(wStruct)\n",
    "        _DocLength[_struct[0]] += distance(0, w) # Cosine similarity 계산용\n",
    "    _WLexicon[t][\"Posting\"] = wptr\n",
    "    _WLexicon[t][\"DF\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 검색하기 (Euclidean distance 이용)\n",
    "- query parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "Q = \"B B B\"\n",
    "result = defaultdict(float)\n",
    "QueryRepr = defaultdict(int) # localPosting가 같은 역할; freq를 센다.\n",
    "QueryWeight = defaultdict(float)\n",
    "for t in Q.split():\n",
    "    if t in _Vocabulary:  # _Vocabulary에 있는 단어만 쿼리로 사용\n",
    "        QueryRepr[t] += 1\n",
    "maxQuery = max(QueryRepr.values())\n",
    "for t, f in QueryRepr.items():\n",
    "    w = tf6(f, maxQuery, 0)*idf2(_WLexicon[t][\"DF\"], N)\n",
    "    QueryWeight[t] = w\n",
    "\n",
    "for t in _Vocabulary:\n",
    "    ptr = _WLexicon[t][\"Posting\"]\n",
    "    df = _WLexicon[t][\"DF\"]\n",
    "    for _struct in _Weight[ptr:ptr+df]:\n",
    "        _struct[0] # => 문서\n",
    "        _struct[1] # => 가중치\n",
    "        result[_struct[0]] += distance(QueryWeight[t],\n",
    "                                      _struct[1])\n",
    "    \n",
    "result = {d:sqrt(dist)  for d, dist in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.3050181911435358, 1: 0.41861327484332994, 0: 0.2876820724517809}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"B B B\"라는 query에 대하여 B가 들어있는 두번째 문서가 가장 높은 유사도를 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 검색하기 (Cosine similarity 이용)\n",
    "- query parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = \"A A A A B\"\n",
    "result = defaultdict(float)\n",
    "QueryRepr = defaultdict(int)\n",
    "QueryWeight = defaultdict(float)\n",
    "QueryLength = 0.0\n",
    "for t in Q.split():\n",
    "    if t in _Vocabulary:\n",
    "        QueryRepr[t] += 1\n",
    "maxQuery = max(QueryRepr.values())\n",
    "for t, f in QueryRepr.items():\n",
    "    w = tf6(f, maxQuery, 0)*idf2(_WLexicon[t][\"DF\"], N)\n",
    "    QueryWeight[t]= w\n",
    "    QueryLength += distance(0,w)\n",
    "\n",
    "for t in QueryRepr:\n",
    "    ptr = _WLexicon[t][\"Posting\"]\n",
    "    df = _WLexicon[t][\"DF\"]\n",
    "    for _struct in _Weight[ptr:ptr+df]:\n",
    "        _struct[0] # => 문서\n",
    "        _struct[1] # => 가중치\n",
    "        \n",
    "        # result를 계산할 때 cosine similarity 수식을 이용한다.\n",
    "        result[_struct[0]] += QueryWeight[t] * _struct[1]    \n",
    "result = {d:ip/(sqrt(QueryLength)*sqrt(_DocLength[d]))\n",
    "          for d, ip in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {2: 0.09303609692847455,\n",
       "             1: 0.09303609692847455,\n",
       "             0: 0.08276097481015171})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_DocLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.889557682904279, 1: 1.0000000000000002, 0: 0.9431636564797644}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Query와 두번째 document가 동일하기 때문에 유사도가 1로 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posting을 local에 저장하는 방법으로 구현하기\n",
    "- 위와 동일한 방법이나, Positing을 local에 저장하여 linked-list를 좀 더 원칙적으로 구현하는 실습.\n",
    "- python open의 tell과 seek 메서드를 pointer로 활용함.\n",
    "- 위의 실습에서 코드가 바뀐 부분은 주석처리하여 차이를 알아보기 쉽게 하였음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struct import pack, unpack\n",
    "\n",
    "_Collections = [\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"A\"],\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"B\"],\n",
    "    [\"A\", \"A\", \"A\",  \"A\", \"C\"],\n",
    "]\n",
    "\n",
    "_Vocabulary = list()\n",
    "_Lexicon = defaultdict(lambda: -1)\n",
    "_Document = defaultdict(int)\n",
    "# _Posting = list()\n",
    "\n",
    "fp = open(\"posting.dat\", \"wb\")\n",
    "\n",
    "for d in _Collections:\n",
    "    _localPosting = defaultdict(int)\n",
    "    for t in d:\n",
    "        if t not in _Vocabulary:\n",
    "            _Vocabulary.append(t)\n",
    "        _localPosting[t] += 1\n",
    "    docID = len(_Document)\n",
    "    _Document[docID] = max(_localPosting.values())\n",
    "    for t, f in _localPosting.items():\n",
    "        ptr = _Lexicon[t]\n",
    "#         nextPtr = len(_Posting)\n",
    "#         _Posting.append((docID, f, ptr))\n",
    "        nextPtr = fp.tell()\n",
    "        fp.write(pack(\"iii\", docID, f, ptr))\n",
    "        _Lexicon[t] = nextPtr\n",
    "        \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 매기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어:A, 문서:2, 빈도:4, 최고빈도:4,         가중치=-0.29\n",
      "단어:A, 문서:1, 빈도:4, 최고빈도:4,         가중치=-0.29\n",
      "단어:A, 문서:0, 빈도:5, 최고빈도:5,         가중치=-0.29\n",
      "단어:B, 문서:1, 빈도:1, 최고빈도:4,         가중치=0.10\n",
      "단어:C, 문서:2, 빈도:1, 최고빈도:4,         가중치=0.10\n"
     ]
    }
   ],
   "source": [
    "N = len(_Collections)\n",
    "_Weight = list()\n",
    "_WLexicon = defaultdict(lambda:{\"Posting\":None, \"DF\":0})\n",
    "_DocLength = defaultdict(float)\n",
    "\n",
    "fp = open(\"posting.dat\", \"rb\")\n",
    "wp = open(\"weight.dat\", \"wb\")\n",
    "\n",
    "for t, ptr in _Lexicon.items():\n",
    "    dfPtr = ptr\n",
    "    df = 0\n",
    "    while dfPtr != -1:\n",
    "#         _struct = _Posting[dfPtr]\n",
    "        fp.seek(dfPtr)\n",
    "        _struct = unpack(\"iii\", fp.read(4*3))\n",
    "        df += 1\n",
    "        dfPtr = _struct[-1]\n",
    "        \n",
    "#     wptr = len(_Weight)\n",
    "    wptr = wp.tell()\n",
    "    while ptr != -1:\n",
    "#         _struct = _Posting[ptr]\n",
    "        fp.seek(ptr)\n",
    "        _struct = unpack(\"iii\", fp.read(4*3))\n",
    "        tf = _struct[1]\n",
    "        maxtf = _Document[_struct[0]]\n",
    "        w = tf6(tf, maxtf, 0)* idf2(df, N)\n",
    "        print(\"단어:{0}, 문서:{1}, 빈도:{2}, 최고빈도:{3}, \\\n",
    "        가중치={4:.2f}\"\n",
    "              .format(\n",
    "                  t, _struct[0], _struct[1],\n",
    "                  maxtf, w))\n",
    "        ptr = _struct[-1]\n",
    "        \n",
    "#         wStruct = (_struct[0], w)\n",
    "#         _Weight.append(wStruct)\n",
    "        wp.write(pack(\"if\", _struct[0], w))\n",
    "        _DocLength[_struct[0]] += distance(0, w)\n",
    "    _WLexicon[t][\"Posting\"] = wptr\n",
    "    _WLexicon[t][\"DF\"] = df\n",
    "    \n",
    "fp.close()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 검색하기 (Euclidean distance 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "Q = \"B B B\"\n",
    "result = defaultdict(float)\n",
    "QueryRepr = defaultdict(int) \n",
    "QueryWeight = defaultdict(float)\n",
    "for t in Q.split():\n",
    "    if t in _Vocabulary:  \n",
    "        QueryRepr[t] += 1\n",
    "maxQuery = max(QueryRepr.values())\n",
    "for t, f in QueryRepr.items():\n",
    "    w = tf6(f, maxQuery, 0)*idf2(_WLexicon[t][\"DF\"], N)\n",
    "    QueryWeight[t] = w\n",
    "\n",
    "wp = open(\"weight.dat\", \"rb\")\n",
    "    \n",
    "for t in _Vocabulary:\n",
    "    ptr = _WLexicon[t][\"Posting\"]\n",
    "    df = _WLexicon[t][\"DF\"]\n",
    "#     for _struct in _Weight[ptr:ptr+df]:\n",
    "    wp.seek(ptr)\n",
    "    for i in range(df):\n",
    "        _struct = unpack(\"if\", wp.read(4*2))\n",
    "        result[_struct[0]] += distance(QueryWeight[t],\n",
    "                                      _struct[1])\n",
    "\n",
    "wp.close()\n",
    "result = {d:sqrt(dist)  for d, dist in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 검색하기 (Cosine similarity 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "Q = \"A A A A B\"\n",
    "result = defaultdict(float)\n",
    "QueryRepr = defaultdict(int)\n",
    "QueryWeight = defaultdict(float)\n",
    "QueryLength = 0.0\n",
    "for t in Q.split():\n",
    "    if t in _Vocabulary:\n",
    "        QueryRepr[t] += 1\n",
    "maxQuery = max(QueryRepr.values())\n",
    "fp = open(\"weight.dat\", \"rb\")\n",
    "for t, f in QueryRepr.items():\n",
    "    w = tf6(f, maxQuery, 0)*idf2(_WLexicon[t][\"DF\"], N)\n",
    "    QueryWeight[t]= w\n",
    "    QueryLength += distance(0,w)\n",
    "\n",
    "for t in QueryRepr:\n",
    "    ptr = _WLexicon[t][\"Posting\"]\n",
    "    df = _WLexicon[t][\"DF\"]\n",
    "    fp.seek(ptr)\n",
    "    for i in range(df):\n",
    "        _struct = unpack(\"if\", fp.read(4*2))\n",
    "        _struct[0] # => 문서\n",
    "        _struct[1] # => 가중치\n",
    "        result[_struct[0]] += QueryWeight[t] * _struct[1]  \n",
    "result = {d:ip/(sqrt(QueryLength)*sqrt(_DocLength[d]))\n",
    "          for d, ip in result.items()}\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.8895577255065847, 1: 1.0000000393683177, 0: 0.9431637016493439}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 corpus로 실전 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JINHYO\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.corpus import kobill\n",
    "from konlpy.tag import Kkma\n",
    "from struct import pack, unpack\n",
    "\n",
    "\n",
    "Lexicon = defaultdict(lambda:{\"ptr\":-1, \"df\":0})\n",
    "# 단어:{위치, 총몇개의문서}\n",
    "\n",
    "Documents = defaultdict(lambda:{\n",
    "    \"length\":0.0, \"ttf\":0, \"max\":0})\n",
    "# 문서:(문서벡터의길이, 총몇개의단어, 이문서에서가장많이나온단어의빈도)\n",
    "# ttf = total term frequency\n",
    "\n",
    "DocumentsList = list()\n",
    "# 인덱스:문서의제목\n",
    "\n",
    "kkma = Kkma()\n",
    "# 형태소분석기(꼬꼬마)\n",
    "\n",
    "fp = open(\"inverted.dat\", \"wb\")\n",
    "# TDM(frequency)\n",
    "\n",
    "for docName in kobill.fileids():\n",
    "    # Local\n",
    "    document = kobill.open(docName).read() # 개별문서\n",
    "    localPostings = defaultdict(int) # 각 문서의 TDM(Vector)\n",
    "    # 문서정보 저장\n",
    "    DocID = len(DocumentsList) # 문서의 제목 -> 숫자\n",
    "    DocumentsList.append(docName) # 해당 숫자(위치)  제목 저장\n",
    "    # 각 문서에서 색인어 추출 방식 (형태소분석기, 형태소의길이로정규화)\n",
    "    for term in [_ for _ in kkma.morphs(document)\n",
    "              if 1 < len(_) < 6]:\n",
    "        localPostings[term] += 1\n",
    "        # 문서 벡터 생성 (열 단어:빈도)\n",
    "    # 문서의 통계정보 저장 => for weight\n",
    "    Documents[DocID][\"ttf\"] = sum(localPostings.values())\n",
    "    Documents[DocID][\"max\"] = max(localPostings.values())\n",
    "    # Global\n",
    "    for term, freq in localPostings.items():\n",
    "            if term not in Lexicon:\n",
    "        ptr = Lexicon[term][\"ptr\"]\n",
    "        # 1. 단어가 첫 등장: 위치 ptr=-1\n",
    "        # 2. 단어가 있을 때, ptr= 마지막 저장 위치\n",
    "        postingPtr= fp.tell()\n",
    "        # 파일의 어느 위치에 저장하는 지 \n",
    "        fp.write(pack(\"iii\", DocID, freq, ptr))\n",
    "        # 구조체를 저장(int, int, int)\n",
    "        Lexicon[term][\"ptr\"] = postingPtr\n",
    "        # 1.1 위치 변경 : 파일의 위치\n",
    "        Lexicon[term][\"df\"] += 1\n",
    "        # for IDF\n",
    "        \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {0: {'length': 0.0, 'ttf': 883, 'max': 123},\n",
       "             1: {'length': 0.0, 'ttf': 883, 'max': 119},\n",
       "             2: {'length': 0.0, 'ttf': 1039, 'max': 200},\n",
       "             3: {'length': 0.0, 'ttf': 893, 'max': 92},\n",
       "             4: {'length': 0.0, 'ttf': 213, 'max': 11},\n",
       "             5: {'length': 0.0, 'ttf': 346, 'max': 35},\n",
       "             6: {'length': 0.0, 'ttf': 1803, 'max': 122},\n",
       "             7: {'length': 0.0, 'ttf': 672, 'max': 35},\n",
       "             8: {'length': 0.0, 'ttf': 642, 'max': 31},\n",
       "             9: {'length': 0.0, 'ttf': 1855, 'max': 430}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexer => Weighting(weight.dat)\n",
    "\n",
    "from math import log2\n",
    "\n",
    "fp1 = open(\"inverted.dat\", \"rb\")\n",
    "fp2 = open(\"weight.dat\", \"wb\")\n",
    "\n",
    "N = len(DocumentsList)\n",
    "for term, termStruct in Lexicon.items():\n",
    "    ptr = termStruct[\"ptr\"]\n",
    "    wPtr = fp2.tell()\n",
    "    while ptr != -1:\n",
    "        fp1.seek(ptr)\n",
    "        _struct = unpack(\"iii\", fp1.read(4*3)) # 12byte\n",
    "        _struct[0] # => 문서 ID\n",
    "        _struct[1] # => 해당 문서에서의 빈도(tf)\n",
    "        TF = _struct[1] / Documents[_struct[0]][\"max\"]\n",
    "        IDF = log2(N/termStruct[\"df\"])\n",
    "        Documents[_struct[0]][\"length\"] += TF*IDF\n",
    "        fp2.write(pack(\"if\", _struct[0], TF*IDF))\n",
    "        ptr = _struct[-1]\n",
    "    Lexicon[term][\"ptr\"] = wPtr\n",
    "fp2.close()\n",
    "fp1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Parser => QyeryRepr(with Weight)\n",
    "\n",
    "Query = \"현행법은 입법예고와 행정예고를 통하여 정책 결정 과정에\"\n",
    "\n",
    "localPostings = defaultdict(int)\n",
    "\n",
    "for term in [_ for _ in kkma.morphs(Query)\n",
    "                if 1 < len(_) < 6]:\n",
    "    localPostings[term] += 1\n",
    "maxTF = max(localPostings.values())\n",
    "# TTF = sum(localPostings.values()) => 나중에 BM25에서 쓸거다.\n",
    "for term, freq, in localPostings.items():\n",
    "    TF = freq/maxTF\n",
    "    IDF = log2(N/Lexicon[term][\"df\"])\n",
    "    localPostings[term] = TF*IDF\n",
    "    \n",
    "# ==> Query Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'현행법': 0.5,\n",
       "             '입법': 1.660964047443681,\n",
       "             '예고': 3.321928094887362,\n",
       "             '행정': 0.3684827970831031,\n",
       "             '통하': 1.160964047443681,\n",
       "             '정책': 0.2572865864148791,\n",
       "             '결정': 1.660964047443681,\n",
       "             '과정': 0.6609640474436812})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localPostings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Euclidean distance\n",
    "- 식:\n",
    "$$\n",
    "dist(q, d) = \\sqrt{\\sum_{t\\in V}{[tf(t,q) \\cdot idf(t) - tf(t,d) \\cdot idf(t)]^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 해석: (시그마 t ㅌ V) sqrt((D벡터t번째값-Q벡터t번째값)\\*\\*2)\n",
    "- 문제: summation을 할때, vocabulary에 속하는 모든 t(단어)에 대해서 실행하는데 t는 query에 없는 단어일수도 있다. 이때 penalty가 발생한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1809892.txt': 0.9785681821197749}\n",
      "{'1809892.txt': {'거리': 0.9785681821197749, '단어수': 0}}\n",
      "교육공무원법 일부개정법률안\n",
      "\n",
      "(정의화의원 대표발의 )\n",
      "\n",
      "{'1809890.txt': 1.1213888599844724}\n",
      "{'1809890.txt': {'거리': 1.1213888599844724, '단어수': 0}}\n",
      "지방공무원법 일부개정법률안\n",
      "\n",
      "(정의화의원 대표발의 )\n",
      "\n",
      "{'1809891.txt': 1.1454561206043383}\n",
      "{'1809891.txt': {'거리': 1.1454561206043383, '단어수': 0}}\n",
      "국가공무원법 일부개정법률안\n",
      "\n",
      "(정의화의원 대표발의 )\n",
      "\n",
      "{'1809893.txt': 1.3363030928544017}\n",
      "{'1809893.txt': {'거리': 1.3363030928544017, '단어수': 0}}\n",
      "남녀고용평등과 일 ·가정 양립 지원에 관한 법률 \n",
      "\n",
      "일부개정법률안\n",
      "\n",
      "{'1809899.txt': 1.5382427458729653}\n",
      "{'1809899.txt': {'거리': 1.5382427458729653, '단어수': 0}}\n",
      "결혼중개업의 관리에 관한 법률 일부개정법률안\n",
      "\n",
      "(한선교의원 대표발의 )\n",
      "\n",
      "{'1809895.txt': 2.3306777198717277}\n",
      "{'1809895.txt': {'거리': 2.3306777198717277, '단어수': 0}}\n",
      "하도급거래 공정화에 관한 법률 일부개정법률안\n",
      "\n",
      "(유선호의원 대표발의 )\n",
      "\n",
      "{'1809896.txt': 3.0498591928696674}\n",
      "{'1809896.txt': {'거리': 3.0498591928696674, '단어수': 0}}\n",
      "행정절차법 일부개정법률안\n",
      "\n",
      "(유선호의원 대표발의 )\n",
      "\n",
      "{'1809898.txt': 4.807353198923258}\n",
      "{'1809898.txt': {'거리': 4.807353198923258, '단어수': 0}}\n",
      "국군부대의 소말리아 해역 파견연장 동의안\n",
      "\n",
      "의안\n",
      "\n",
      "{'1809897.txt': 5.050448921893771}\n",
      "{'1809897.txt': {'거리': 5.050448921893771, '단어수': 0}}\n",
      "국군부대의 아랍에미리트(UAE)군 교육훈련 지원 등에 \n",
      "관한 파견 동의안\n",
      "\n",
      "\n",
      "{'1809894.txt': 6.478089528366782}\n",
      "{'1809894.txt': {'거리': 6.478089528366782, '단어수': 0}}\n",
      "고등교육법 일부개정법률안\n",
      "\n",
      "(안상수의원 대표발의 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "searchResult = defaultdict(float)\n",
    "fp = open(\"weight.dat\", \"rb\")\n",
    "for term, termStruct in Lexicon.items():\n",
    "    ptr = termStruct[\"ptr\"]\n",
    "    fp.seek(ptr)\n",
    "    for _ in range(termStruct[\"df\"]):\n",
    "        _struct = unpack(\"if\", fp.read(4*2))\n",
    "        qw = localPostings[term] # 이름은 나중에 바꾸시구여\n",
    "        dw = _struct[1]\n",
    "        searchResult[_struct[0]] += (qw-dw)**2\n",
    "fp.close()\n",
    "\n",
    "searchResult = {d:sqrt(dist)\n",
    "               for d, dist in searchResult.items()}\n",
    "\n",
    "for d, dist in {DocumentsList[_[0]]:_[1]\n",
    "                for _ in sorted(searchResult.items(), \n",
    "                        key=lambda r:r[1])}.items():\n",
    "    print({d:dist})\n",
    "    print({d:{\"거리\":dist, \"단어수\": Documents[d][\"ttf\"]}})\n",
    "    print(\"\\n\".join(kobill.open(d).read().splitlines()[:3]))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- euclidean distance와 다르게 summation할때 다르다. \n",
    "    - 𝑡∈𝑞∩𝑑\n",
    "\n",
    "- svm은 l1, l2를 고를 필요가 없다.\n",
    "\n",
    "- 보통 개발할 때 수집/학습 서버 1대, 서비스 제공 서버 1대 해서 총 2대를 쓴다. (실시간처럼 느끼게 한다.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: {'각도': 7.431129331674336, '단어수': 1803}}\n",
      "행정절차법 일부개정법률안\n",
      "\n",
      "(유선호의원 대표발의 )\n",
      "{4: {'각도': 0.1718470144736527, '단어수': 213}}\n",
      "고등교육법 일부개정법률안\n",
      "\n",
      "(안상수의원 대표발의 )\n",
      "{8: {'각도': 0.04033460946903825, '단어수': 642}}\n",
      "국군부대의 소말리아 해역 파견연장 동의안\n",
      "\n",
      "의안\n",
      "{7: {'각도': 0.0375486104695337, '단어수': 672}}\n",
      "국군부대의 아랍에미리트(UAE)군 교육훈련 지원 등에 \n",
      "관한 파견 동의안\n",
      "\n",
      "{3: {'각도': 0.02805257664723038, '단어수': 893}}\n",
      "남녀고용평등과 일 ·가정 양립 지원에 관한 법률 \n",
      "\n",
      "일부개정법률안\n",
      "{1: {'각도': 0.01849454212767377, '단어수': 883}}\n",
      "국가공무원법 일부개정법률안\n",
      "\n",
      "(정의화의원 대표발의 )\n",
      "{0: {'각도': 0.018212451826866254, '단어수': 883}}\n",
      "지방공무원법 일부개정법률안\n",
      "\n",
      "(정의화의원 대표발의 )\n",
      "{2: {'각도': 0.012825953697034324, '단어수': 1039}}\n",
      "교육공무원법 일부개정법률안\n",
      "\n",
      "(정의화의원 대표발의 )\n",
      "{9: {'각도': 0.012368922287890492, '단어수': 1855}}\n",
      "결혼중개업의 관리에 관한 법률 일부개정법률안\n",
      "\n",
      "(한선교의원 대표발의 )\n",
      "{5: {'각도': 0.0, '단어수': 346}}\n",
      "하도급거래 공정화에 관한 법률 일부개정법률안\n",
      "\n",
      "(유선호의원 대표발의 )\n"
     ]
    }
   ],
   "source": [
    "# Relevance(Euclidean, Cosine Theta)\n",
    "# 2. Cosine Theta\n",
    "searchResult = defaultdict(float)\n",
    "fp = open(\"weight.dat\", \"rb\")\n",
    "queryLength = 0.0\n",
    "for term, qw in localPostings.items():\n",
    "    ptr = Lexicon[term][\"ptr\"]\n",
    "    fp.seek(ptr)\n",
    "    for _ in range(Lexicon[term][\"df\"]):\n",
    "        _struct = unpack(\"if\", fp.read(4*2))\n",
    "        dw = _struct[1]\n",
    "        searchResult[_struct[0]] += (qw*dw)\n",
    "    queryLength += qw**2\n",
    "fp.close()\n",
    "\n",
    "searchResult = {d:angle/sqrt(Documents[d][\"length\"])\\\n",
    "                            *sqrt(queryLength) # 빼도 무방하지만 빼면 값이 1을 넘어감\n",
    "               for d, angle in searchResult.items()}\n",
    "\n",
    "for d, dist in {_[0]:_[1]\n",
    "                for _ in sorted(searchResult.items(), \n",
    "                                key=lambda r:r[1], \n",
    "                                reverse=True)}.items():\n",
    "    print({d:{\"각도\":dist, \"단어수\": Documents[d][\"ttf\"]}})\n",
    "    print(\"\\n\".join(kobill.open(DocumentsList[d]).read().\\\n",
    "                     splitlines()[:3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ptr': 6168, 'df': 5}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon[\"현행법\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
